{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as tc\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Getting our datsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please do not run the code below more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"DATA\", train=True, download=True, transform=ToTensor(), target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"DATA\", train=False, download=True, transform=ToTensor(), target_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please do not run the code above more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n",
    "# seeing the first data\n",
    "image,label = train_data[0]\n",
    "image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames = train_data.classes\n",
    "classnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = train_data.class_to_idx\n",
    "class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Visualising random sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Image Shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze())\n",
    "plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gray scale\n",
    "plt.imshow(image.squeeze(),cmap=\"gray\")\n",
    "plt.title(classnames[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising random images\n",
    "tc.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "rows,cols = 4,4\n",
    "for i in range(1,rows*cols+1):\n",
    "  random_idx = tc.randint(0,len(train_data),size=[1]).item()\n",
    "  # print(random_idx)\n",
    "  img,lab = train_data[random_idx]\n",
    "  fig.add_subplot(rows,cols,i)\n",
    "  plt.imshow(img.squeeze(),cmap=\"gray\")\n",
    "  plt.title(classnames[lab])\n",
    "  plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Preparing Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# batch size hyper parameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# turn dataset into iterable\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DataLoaders: {train_dataloader,test_dataloader}\")\n",
    "print(f\"Length of train data loader: {len(train_dataloader)} batches of {BATCH_SIZE}...\")\n",
    "print(f\"Length of test data loader: {len(test_dataloader)} batches of {BATCH_SIZE}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking training data loader\n",
    "train_features_batch, train_label_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape,train_label_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample\n",
    "tc.manual_seed(42)\n",
    "random_idx = tc.randint(0,len(train_features_batch),size=[1]).item()\n",
    "img, lab = train_features_batch[random_idx],train_label_batch[random_idx]\n",
    "plt.imshow(img.squeeze(),cmap=\"gray\")\n",
    "plt.title(classnames[lab])\n",
    "plt.axis(False)\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {lab}, Label size: {lab.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a baseline model\n",
    "flatten_model = nn.Flatten()\n",
    "\n",
    "# get a single sample\n",
    "x = train_features_batch[0]\n",
    "x.shape\n",
    "# flatten x\n",
    "output = flatten_model(x)\n",
    "print(f\"Shape before flattening: {x.shape}\\nShape after flattening: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelV0(nn.Module):\n",
    "  def __init__(self,input_shape:int,hidden_unit:int,output_shpe:int):\n",
    "    super().__init__()\n",
    "    self.layer_stack = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(in_features=input_shape,out_features=hidden_unit),\n",
    "      nn.Linear(in_features=hidden_unit,out_features=output_shpe)\n",
    "    )\n",
    "    \n",
    "  def forward(self,x):\n",
    "    return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.manual_seed(42)\n",
    "model0 = FashionMNISTModelV0(input_shape=784,hidden_unit=10,output_shpe=len(classnames))\n",
    "\n",
    "model0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Setup loss function and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy functions\n",
    "from helper import accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = tc.optim.SGD(params=model0.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up timer function\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "def print_train_time(start: float, end: float, devive: tc.device = None):\n",
    "    \"\"\"Prints difference between start time and end time\"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {devive}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Training loop for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = tc.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# set the seed\n",
    "tc.manual_seed(42)\n",
    "start_time = timer()\n",
    "\n",
    "# set epochs\n",
    "epochs = 3\n",
    "\n",
    "# create training and test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print(f\"Epoch: {epoch}\\n-----\")\n",
    "  # training\n",
    "  train_loss = 0\n",
    "  # add a loop to loop through the training batches\n",
    "  for batch,(X,y) in enumerate(train_dataloader):\n",
    "    model0.train()\n",
    "    # 1. forward pass\n",
    "    y_pred = model0(X)\n",
    "    # 2. calculate the loss\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    train_loss +=loss\n",
    "    # 3. optimise the zero grad\n",
    "    optimiser.zero_grad()\n",
    "    # 4. loss backward\n",
    "    loss.backward()\n",
    "    # optimise the step\n",
    "    optimiser.step()\n",
    "  \n",
    "    # print what is happening\n",
    "    if batch % 400 == 0:\n",
    "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "  # Divide total train loss by the lenght of the train dataloader\n",
    "  train_loss /= len(train_dataloader)\n",
    "  \n",
    "  ### Testing\n",
    "  test_loss,test_acc = 0,0\n",
    "  model0.eval()\n",
    "  with tc.inference_mode():\n",
    "    for X_test,y_test in test_dataloader:\n",
    "      # 1. forward pass\n",
    "      test_pred = model0(X_test)\n",
    "      # 2. calculate the loss\n",
    "      test_loss += loss_fn(test_pred,y_test)\n",
    "      # 3. calculate accuracy\n",
    "      test_acc += accuracy_fn(y_true=y_test,y_pred=test_pred.argmax(dim=1))\n",
    "    # calculate the test loss avg per batch\n",
    "    test_loss /= len(test_dataloader)\n",
    "    # calculate the test acc avg per batch\n",
    "    test_acc /= len(test_dataloader)\n",
    "    \n",
    "  # print what is happening\n",
    "  print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.3f}\")\n",
    "  \n",
    "# Calculate the training time\n",
    "end_time = timer()\n",
    "total_time0 = print_train_time(start=start_time,end=end_time,devive=str(next(model0.parameters()).device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making some predictions\n",
    "tc.manual_seed(42)\n",
    "def eval_model(model:tc.nn.Module,data_loader:tc.utils.data.DataLoader,loss_fn:tc.nn.Module,accuracy_fn):\n",
    "  \"\"\"Return a disctionary containing the results of model predicting on data loader.\"\"\"\n",
    "  \n",
    "  loss, acc = 0,0\n",
    "  model.eval()\n",
    "  with tc.inference_mode():\n",
    "    for X,y in data_loader:\n",
    "      # make predictions\n",
    "      y_pred = model(X)\n",
    "      \n",
    "      # accumulate the loss and acc values per patch\n",
    "      loss += loss_fn(y_pred,y)\n",
    "      acc += accuracy_fn(y_true=y,y_pred=y_pred.argmax(dim=1))\n",
    "    # scalle the loss and acc\n",
    "    loss /= len(data_loader)\n",
    "    acc /= len(data_loader)\n",
    "    \n",
    "  return {\"Model name\": model.__class__.__name__,\"Model Loss\": loss.item(),\"Model acc\": acc}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model 0 result on our dataset\n",
    "model0_results = eval_model(model=model0,data_loader=test_dataloader,loss_fn=loss_fn,accuracy_fn=accuracy_fn)\n",
    "\n",
    "model0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Improving through experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Building a better model with non linearlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelV1(nn.Module):\n",
    "  def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
    "    super().__init__()\n",
    "    self.layer_stack = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(in_features=hidden_units,out_features=output_shape),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "  \n",
    "  def forward(self,x:tc.Tensor):\n",
    "    return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instanc of model 1\n",
    "tc.manual_seed(42)\n",
    "model1 = FashionMNISTModelV1(input_shape=784,hidden_units=10,output_shape=len(classnames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1.1 Setting up loss function and optimizer and a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn1 = nn.CrossEntropyLoss()\n",
    "optimiser1 = tc.optim.SGD(params=model1.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the seed\n",
    "tc.manual_seed(42)\n",
    "start_time = timer()\n",
    "\n",
    "# set epochs\n",
    "epochs = 3\n",
    "\n",
    "# create training and test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-----\")\n",
    "    # training\n",
    "    train_loss = 0\n",
    "    # add a loop to loop through the training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model1.train()\n",
    "        # 1. forward pass\n",
    "        y_pred = model0(X)\n",
    "        # 2. calculate the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        # 3. optimise the zero grad\n",
    "        optimiser1.zero_grad()\n",
    "        # 4. loss backward\n",
    "        loss.backward()\n",
    "        # optimise the step\n",
    "        optimiser1.step()\n",
    "\n",
    "        # print what is happening\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by the lenght of the train dataloader\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    ### Testing\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model1.eval()\n",
    "    with tc.inference_mode():\n",
    "        for X_test, y_test in test_dataloader:\n",
    "            # 1. forward pass\n",
    "            test_pred = model1(X_test)\n",
    "            # 2. calculate the loss\n",
    "            test_loss += loss_fn(test_pred, y_test)\n",
    "            # 3. calculate accuracy\n",
    "            test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
    "        # calculate the test loss avg per batch\n",
    "        test_loss /= len(test_dataloader)\n",
    "        # calculate the test acc avg per batch\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "        # print what is happening\n",
    "        print(\n",
    "            f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "# Calculate the training time\n",
    "end_time = timer()\n",
    "total_time1 = print_train_time(\n",
    "    start=start_time, end=end_time, devive=str(next(model1.parameters()).device)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1.2 Functionising raining anf evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model:tc.nn.Module,data_loader:tc.utils.data.DataLoader,loss_fn:tc.nn.Module,optimiser:tc.optim.Optimizer,accuracy_fn):\n",
    "  \"\"\"Performs training step with model trying to learn on data loader\"\"\"\n",
    "  \n",
    "  train_loss,train_acc = 0,0\n",
    "  # put model in training mode\n",
    "  model.train()\n",
    "  for batch,(X,y) in enumerate(data_loader):\n",
    "    # 1. forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # 2. calculate the loss and acc per batch\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    train_loss += loss\n",
    "    train_acc += accuracy_fn(y_true=y,y_pred=y_pred.argmax(dim=1))\n",
    "    \n",
    "    # 3. optimize the zero grad\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. optimize the step\n",
    "    optimiser.step()\n",
    "    \n",
    "    # print out whats happening\n",
    "    # if batch % 400 == 0:\n",
    "    #   print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "      \n",
    "  # Divide total train loss and acc by length of train data loader\n",
    "  train_loss /= len(data_loader)\n",
    "  train_acc /= len(data_loader)\n",
    "  print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model:tc.nn.Module,data_loader:tc.utils.data.DataLoader,loss_fn:tc.nn.Module,accuracy_fn):\n",
    "  \"\"\"Performs testing step with model trying to learn on data loader\"\"\"\n",
    "  ### Testing\n",
    "  test_loss, test_acc = 0, 0\n",
    "  model.eval()\n",
    "  with tc.inference_mode():\n",
    "    for X_test, y_test in data_loader:\n",
    "      # 1. forward pass\n",
    "      test_pred = model(X_test)\n",
    "      # 2. calculate the loss\n",
    "      test_loss += loss_fn(test_pred, y_test)\n",
    "      # 3. calculate accuracy\n",
    "      test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
    "    # calculate the test loss avg per batch\n",
    "    test_loss /= len(data_loader)\n",
    "    # calculate the test acc avg per batch\n",
    "    test_acc /= len(data_loader)\n",
    "\n",
    "    # print what is happening\n",
    "    print(\n",
    "      f\"\\nTest loss: {test_loss:.4f}, Test acc: {test_acc:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.manual_seed(42)\n",
    "# measuring the time\n",
    "start_time1 = timer()\n",
    "\n",
    "# set epochs\n",
    "epchs = 3\n",
    "# create optimization and evaluation hooks\n",
    "for epch in tqdm(range(epchs)):\n",
    "    print(f\"Epoch: {epch}\")\n",
    "    train_step(\n",
    "        model=model1,\n",
    "        data_loader=train_dataloader,\n",
    "        loss_fn=loss_fn,\n",
    "        optimiser=optimiser1,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "    )\n",
    "    test_step(\n",
    "        model=model1,\n",
    "        data_loader=test_dataloader,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "    )\n",
    "    \n",
    "end_time1 = timer()\n",
    "total_time1 = print_train_time(start=start_time1,end=end_time1,devive=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model0_results)\n",
    "print(total_time0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model1 eval dictionary\n",
    "model1_results = eval_model(model=model1,data_loader=test_dataloader,loss_fn=loss_fn,accuracy_fn=accuracy_fn)\n",
    "model1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Improving our model with Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a CNN\n",
    "class FashionMNISTModelV2(nn.Module):\n",
    "  \"\"\"\n",
    "  Model architechture that replicate a TinyVGG\n",
    "  \"\"\"\n",
    "  def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
    "    super().__init__()\n",
    "    self.conv_block_1 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=input_shape,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2)\n",
    "    )\n",
    "    \n",
    "    self.conv_block_2 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2)\n",
    "    )\n",
    "    \n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(in_features=hidden_units*0,out_features=output_shape)\n",
    "    )\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = self.conv_block_1(x)\n",
    "    print(x.shape)\n",
    "    x = self.conv_block_2(x)\n",
    "    print(x.shape)\n",
    "    x = self.classifier(x)\n",
    "    print(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.manual_seed(42)\n",
    "model2 = FashionMNISTModelV2(input_shape=1,hidden_units=10,output_shape=len(classnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2.1 Stepping through nn.Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
